{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ee1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "MPS device not found.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from Wordle import WordleEnv\n",
    "from models.DQN import DQN\n",
    "from models.ActorCritic import Actor, Critic\n",
    "from ReplayMemory import ReplayMemory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Initialize the environment (input subset size if necessary)\n",
    "size = None\n",
    "env = WordleEnv(subset_size=size) \n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab92040",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 150000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "n_actions = env.action_size\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(env.state_size, env.action_size).to(device)\n",
    "target_net = DQN(env.state_size, env.action_size).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Optimizer initialization\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "# Epsilon greedy action selection\n",
    "# Gradually decrease epsilon\n",
    "# If epsilon is greater than the random sample, take random action\n",
    "# Otherwise, take the action that gives the most Q value.\n",
    "def select_action(state, available_actions, action_size):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    global eps_threshold\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Create a mask tensor for previously chosen actions\n",
    "            mask = torch.full((1, action_size), -float('inf'), device=device)\n",
    "            for idx in available_actions:\n",
    "                mask[0, idx] = 0\n",
    "\n",
    "            # Add the mask to the DQN output and select the maximum value\n",
    "            masked_output = policy_net(state) + mask\n",
    "            return masked_output.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(available_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a412a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb479d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/300000, Attempts: 6, Reward: 10\n",
      "Episode: 1000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 2000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 3000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 4000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 5000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 6000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 7000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 8000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 9000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 10000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 11000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 12000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 13000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 14000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 15000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 16000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 17000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 18000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 19000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 20000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 21000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 22000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 23000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 24000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 25000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 26000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 27000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 28000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 29000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 30000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 31000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 32000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 33000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 34000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 35000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 36000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 37000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 38000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 39000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 40000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 41000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 42000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 43000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 44000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 45000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 46000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 47000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 48000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 49000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 50000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 51000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 52000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 53000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 54000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 55000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 56000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 57000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 58000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 59000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 60000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 61000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 62000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 63000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 64000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 65000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 66000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 67000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 68000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 69000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 70000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 71000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 72000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 73000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 74000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 75000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 76000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 77000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 78000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 79000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 80000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 81000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 82000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 83000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 84000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 85000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 86000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 87000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 88000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 89000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 90000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 91000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 92000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 93000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 94000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 95000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 96000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 97000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 98000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 99000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 100000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 101000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 102000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 103000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 104000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 105000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 106000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 107000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 108000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 109000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 110000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 111000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 112000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 113000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 114000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 115000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 116000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 117000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 118000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 119000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 120000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 121000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 122000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 123000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 124000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 125000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 126000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 127000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 128000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 129000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 130000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 131000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 132000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 133000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 134000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 135000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 136000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 137000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 138000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 139000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 140000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 141000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 142000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 143000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 144000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 145000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 146000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 147000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 148000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 149000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 150000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 151000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 152000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 153000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 154000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 155000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 156000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 157000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 158000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 159000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 160000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 161000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 162000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 163000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 164000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 165000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 166000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 167000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 168000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 169000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 170000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 171000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 172000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 173000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 174000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 175000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 176000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 177000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 178000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 179000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 180000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 181000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 182000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 183000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 184000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 185000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 186000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 187000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 188000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 189000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 190000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 191000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 192000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 193000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 194000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 195000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 196000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 197000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 198000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 199000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 200000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 201000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 202000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 203000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 204000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 205000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 206000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 207000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 208000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 209000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 210000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 211000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 212000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 213000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 214000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 215000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 216000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 217000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 218000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 219000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 220000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 221000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 222000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 223000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 224000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 225000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 226000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 227000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 228000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 229000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 230000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 231000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 232000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 233000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 234000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 235000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 236000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 237000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 238000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 239000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 240000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 241000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 242000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 243000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 244000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 245000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 246000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 247000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 248000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 249000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 250000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 251000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 252000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 253000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 254000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 255000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 256000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 257000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 258000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 259000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 260000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 261000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 262000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 263000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 264000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 265000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 266000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 267000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 268000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 269000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 270000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 271000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 272000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 273000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 274000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 275000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 276000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 277000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 278000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 279000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 280000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 281000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 282000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 283000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 284000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 285000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 286000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 287000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 288000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 289000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 290000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 291000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 292000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 293000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 294000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 295000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 296000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 297000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 298000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 299000/300000, Attempts: 6, Reward: -10\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300000\n",
    "average_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            average_reward += reward/1000\n",
    "            if(episode % 1000 == 0):\n",
    "                print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Reward: {reward[0]}\")\n",
    "                average_reward = 0\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "332ea6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials: 1000, Success rate: 0.66, Average number of attempts: 5.09\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bceb60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials with SALET start: 1000, Success rate: 0.71, Average number of attempts: 4.95\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    action = torch.tensor([[345]], device=device, dtype=torch.long) #Salet start.\n",
    "    for t in count():\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials with SALET start: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "32723e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SLOSH\n",
      "Target word: THORN\n",
      "Attempts left: 5\n",
      "Current guess: BOOZY\n",
      "Target word: THORN\n",
      "Attempts left: 4\n",
      "Current guess: ATOLL\n",
      "Target word: THORN\n",
      "Attempts left: 3\n",
      "Current guess: ERODE\n",
      "Target word: THORN\n",
      "Attempts left: 2\n",
      "Current guess: THORN\n",
      "Target word: THORN\n",
      "Attempts left: 2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c73c7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: FRONT\n",
      "Attempts left: 5\n",
      "Current guess: POINT\n",
      "Target word: FRONT\n",
      "Attempts left: 4\n",
      "Current guess: CHANT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n",
      "Current guess: FRONT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'FRONT'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "78feac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: CHOKE\n",
      "Attempts left: 5\n",
      "Current guess: KEBAB\n",
      "Target word: CHOKE\n",
      "Attempts left: 4\n",
      "Current guess: TRAWL\n",
      "Target word: CHOKE\n",
      "Attempts left: 3\n",
      "Current guess: ACUTE\n",
      "Target word: CHOKE\n",
      "Attempts left: 2\n",
      "Current guess: DODGE\n",
      "Target word: CHOKE\n",
      "Attempts left: 1\n",
      "Current guess: ELOPE\n",
      "Target word: CHOKE\n",
      "Attempts left: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'CHOKE'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980ab1e",
   "metadata": {},
   "source": [
    "## Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(env.state_size, env.action_size).to(device)\n",
    "critic = Critic(env.state_size, env.action_size).to(device)\n",
    "\n",
    "def select_action_actor(state, available_actions, action_size):\n",
    "    with torch.no_grad():\n",
    "        # Create a mask tensor for previously chosen actions\n",
    "        mask = torch.full((1, action_size), -float('inf'), device=device)\n",
    "        for idx in available_actions:\n",
    "            mask[0, idx] = 0\n",
    "\n",
    "        # Add the mask to the actor output and sample from the distribution\n",
    "        actor_output = actor(state).probs\n",
    "        masked_output = actor_output + mask\n",
    "        masked_distribution = Categorical(logits=masked_output)\n",
    "        return masked_distribution.sample().view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = 0\n",
    "num_episodes = 10000\n",
    "\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=0.001)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "\n",
    "    for i in count():\n",
    "        dist, value = actor(state), critic(state)\n",
    "\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        log_prob = dist.log_prob(action).unsqueeze(1)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        if value.dim() > 1:\n",
    "            values.append(value.squeeze(0))\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            average_reward += reward/1000\n",
    "            if(episode % 1000 == 0):\n",
    "                print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Reward: {reward}\")\n",
    "                average_reward = 0\n",
    "            break\n",
    "\n",
    "    next_value = critic(next_state)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    returns = torch.tensor(rewards).sum()\n",
    "    if values:\n",
    "        values = torch.cat(values, dim=0).unsqueeze(1)\n",
    "    else:\n",
    "        # If values is empty, initialize it with a dummy tensor to avoid errors\n",
    "        values = torch.zeros(1, 1, device=device, requires_grad=True)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    optimizer_actor.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    critic_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    action = torch.tensor([[345]], device=device, dtype=torch.long) #Salet start.\n",
    "    for t in count():\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials with SALET start: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5839904",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'OTHER'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e07c65",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic (A3C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
